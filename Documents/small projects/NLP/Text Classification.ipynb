{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import string\n",
    "import random\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Conceptnet Numberbatch word embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Download word vectors\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "if not os.path.isfile('mini.h5'):\n",
    "    print(\"Downloading Conceptnet Numberbatch word embeddings...\")\n",
    "    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n",
    "    urlretrieve(conceptnet_url, 'mini.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "with h5py.File('mini.h5', 'r') as f:\n",
    "    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
    "    all_embeddings = f['mat']['block0_values'][:]\n",
    "    \n",
    "english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
    "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
    "english_embedddings = all_embeddings[english_word_indices]\n",
    "\n",
    "norms = np.linalg.norm(english_embedddings, axis=1)\n",
    "normalized_embeddings = english_embedddings.astype('float32') / norms.astype('float32').reshape([-1, 1])\n",
    "\n",
    "index = {word: i for i, word in enumerate(english_words)}\n",
    "\n",
    "def similarity_score(w1, w2):\n",
    "    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])\n",
    "    return score\n",
    "\n",
    "def print_similarity(w1,w2):\n",
    "    try:\n",
    "        print('{0}\\t{1}\\t'.format(w1,w2), \\\n",
    "          similarity_score('{}'.format(w1), '{}'.format(w2)))\n",
    "    except:\n",
    "        print('One of the words is not in the dictionary.')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punct=str.maketrans('','',string.punctuation)\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 300-dimensional representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_example(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words\n",
    "                  if w in index]\n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return {'x': x, 'y': y, 'w':embeddings}\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "enc = 'utf-8' # This is necessary from within the singularity shell\n",
    "with open(\"movie-simple.txt\", \"r\", encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(dataset)\n",
    "\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size\n",
    "\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_acc=[]\n",
    "results_param=[]\n",
    "for lr in [0.0005, 0.005, 0.05, 0.1]:\n",
    "    for layer_size_1 in [25, 50,100,150,200]:\n",
    "        for layer_size_2 in [25, 50,100,150,200]:\n",
    "            tf.reset_default_graph()\n",
    "\n",
    "            # Placeholders for input\n",
    "            X = tf.placeholder(tf.float32, [None, 300])\n",
    "            y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "            # Three-layer MLP\n",
    "            h1 = tf.layers.dense(X, layer_size_1, tf.nn.relu, kernel_initializer=tf.initializers.truncated_normal)\n",
    "            h2 = tf.layers.dense(h1, layer_size_2, tf.nn.relu, kernel_initializer=tf.initializers.truncated_normal)\n",
    "            logits = tf.layers.dense(h2, 1)\n",
    "            probabilities = tf.sigmoid(logits)\n",
    "\n",
    "            # Loss and metrics\n",
    "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(logits)), y), tf.float32))\n",
    "\n",
    "            # Training\n",
    "            train_step = tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "\n",
    "            # Initialization of variables\n",
    "            initialize_all = tf.global_variables_initializer()\n",
    "            sess = tf.InteractiveSession()\n",
    "            sess.run(initialize_all)\n",
    "            for epoch in range(500):\n",
    "                for batch in range(train_batches):\n",
    "                    data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "                    reviews = [sample['x'] for sample in data]\n",
    "                    labels  = [sample['y'] for sample in data]\n",
    "                    labels = np.array(labels).reshape([-1, 1])\n",
    "                    _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "                if epoch % 10 == 0:\n",
    "                    print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
    "                random.shuffle(train)\n",
    "\n",
    "            # Evaluate on test set\n",
    "            test_reviews = [sample['x'] for sample in test]\n",
    "            test_labels  = [sample['y'] for sample in test]\n",
    "            test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "            acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "            print(\"Final accuracy:\", acc)\n",
    "            results_acc.append(acc)\n",
    "            results_param.append([lr, layer_size_1, layer_size_2])\n",
    "argmax=np.argmax(results_acc)\n",
    "print ('Max accuracy: ', results_acc[argmax])\n",
    "print ('max parameters: ', results_param[argmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the hyperparameters the accuracy was 0.95.\n",
    "\n",
    "Size of hidden layer 1: 25\n",
    "Size of hidden layer 2: 50\n",
    "Learning rate: 0.1\n",
    "\n",
    "\n",
    "Accuracy: 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"movie-simple.txt\", \"r\",encoding=enc) as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]\n",
    "import random\n",
    "random.shuffle(dataset)\n",
    "batch_size = 1\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1714: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 99 Loss 0.6890430946503397 Acc 0.5679267292680439\n",
      "batch 199 Loss 0.6452920381140875 Acc 0.637673310413806\n",
      "batch 299 Loss 0.5929292699282466 Acc 0.7139748132471735\n",
      "batch 399 Loss 0.5777827962740152 Acc 0.6991921835980426\n",
      "batch 499 Loss 0.45764420688484003 Acc 0.7995061641592535\n",
      "batch 599 Loss 0.44890900851499205 Acc 0.8242916622930305\n",
      "batch 699 Loss 0.37686708342248737 Acc 0.8656283694864586\n",
      "batch 799 Loss 0.3355433456304097 Acc 0.8689760916827961\n",
      "batch 899 Loss 0.2987315440251814 Acc 0.9054652230764809\n",
      "batch 999 Loss 0.22958673844969288 Acc 0.9265061950572931\n",
      "Epoch 0 Loss 0.23755461405959188 Acc 0.9308909410781158\n",
      "batch 99 Loss 0.1834649188244278 Acc 0.9530707386449337\n",
      "batch 199 Loss 0.2136503709762892 Acc 0.9371769508088312\n",
      "batch 299 Loss 0.24893887145855337 Acc 0.9093194403831523\n",
      "batch 399 Loss 0.18573083285803635 Acc 0.9361814622153067\n",
      "batch 499 Loss 0.18992625614935113 Acc 0.9361551760904817\n",
      "batch 599 Loss 0.2196896773019936 Acc 0.9102254590929971\n",
      "batch 699 Loss 0.1604769751720936 Acc 0.9413733858601384\n",
      "batch 799 Loss 0.20390322777968145 Acc 0.935102573919765\n",
      "batch 899 Loss 0.18192122167410169 Acc 0.9454128332840288\n",
      "batch 999 Loss 0.1200415670988414 Acc 0.9661962118087507\n",
      "Epoch 1 Loss 0.16018467323195398 Acc 0.9431791256093071\n",
      "batch 99 Loss 0.1429953545954444 Acc 0.9425441779702475\n",
      "batch 199 Loss 0.16385690878010156 Acc 0.9167160620967613\n",
      "batch 299 Loss 0.12205638182815448 Acc 0.9405347488923071\n",
      "batch 399 Loss 0.11128394429422983 Acc 0.9534051094435068\n",
      "batch 499 Loss 0.15878399260947842 Acc 0.9424490507337673\n",
      "batch 599 Loss 0.10960245816480846 Acc 0.9738346338350887\n",
      "batch 699 Loss 0.10886267742350507 Acc 0.9848957949907622\n",
      "batch 799 Loss 0.13245109690617932 Acc 0.9564548850973116\n",
      "batch 899 Loss 0.13253718426295402 Acc 0.9481428174004602\n",
      "batch 999 Loss 0.20883321207101807 Acc 0.9307209392707181\n",
      "Epoch 2 Loss 0.18681202825071122 Acc 0.9214903002412129\n",
      "batch 99 Loss 0.12419947451671616 Acc 0.9597532016969987\n",
      "batch 199 Loss 0.10028050494720249 Acc 0.9658130809272275\n",
      "batch 299 Loss 0.0832438823309133 Acc 0.9813135805614516\n",
      "batch 399 Loss 0.0910808415452716 Acc 0.970390874345784\n",
      "batch 499 Loss 0.1357735974917524 Acc 0.9387051301859348\n",
      "batch 599 Loss 0.11529580755114796 Acc 0.9570260320479558\n",
      "batch 699 Loss 0.15267519835461357 Acc 0.9502997081248971\n",
      "batch 799 Loss 0.15196168337291108 Acc 0.9462372711719468\n",
      "batch 899 Loss 0.18179176687333315 Acc 0.928459607349251\n",
      "batch 999 Loss 0.10681361965846141 Acc 0.9637361407735532\n",
      "Epoch 3 Loss 0.12910070272825913 Acc 0.9541065298137447\n",
      "batch 99 Loss 0.08478759503079673 Acc 0.9651354814369492\n",
      "batch 199 Loss 0.08378494704253155 Acc 0.9667881319844313\n",
      "batch 299 Loss 0.09772983565955 Acc 0.9622294691616203\n",
      "batch 399 Loss 0.13144267750242428 Acc 0.9561337046227636\n",
      "batch 499 Loss 0.11226161444456266 Acc 0.9703683224591269\n",
      "batch 599 Loss 0.12809695993125547 Acc 0.9628232988259235\n",
      "batch 699 Loss 0.14903478206353069 Acc 0.9579648373185836\n",
      "batch 799 Loss 0.10340861751487253 Acc 0.9761910801326938\n",
      "batch 899 Loss 0.09394323254149978 Acc 0.9762343824723971\n",
      "batch 999 Loss 0.10474284867865025 Acc 0.9634453983803992\n",
      "Epoch 4 Loss 0.11016997819308023 Acc 0.9527007437397153\n",
      "Final accuracy: 0.9518413597733711\n",
      "batch 99 Loss 0.6675486691836812 Acc 0.6082085054961455\n",
      "batch 199 Loss 0.6010300614566674 Acc 0.6560637224645406\n",
      "batch 299 Loss 0.6210027918680725 Acc 0.685925095113799\n",
      "batch 399 Loss 0.5994940502119627 Acc 0.6914220877517621\n",
      "batch 499 Loss 0.5944244969506045 Acc 0.685531985796954\n",
      "batch 599 Loss 0.4989124314584307 Acc 0.7592507787295916\n",
      "batch 699 Loss 0.5878130083182532 Acc 0.6898163795586699\n",
      "batch 799 Loss 0.4442342920319725 Acc 0.8027403238336549\n",
      "batch 899 Loss 0.5097241298324329 Acc 0.7505782477575323\n",
      "batch 999 Loss 0.5133613169227794 Acc 0.7768360168745609\n",
      "Epoch 0 Loss 0.45133240925719875 Acc 0.8213037691899916\n",
      "batch 99 Loss 0.37956064714099824 Acc 0.856943521371094\n",
      "batch 199 Loss 0.5124607657483665 Acc 0.7842904229438715\n",
      "batch 299 Loss 0.5216208432355526 Acc 0.7564408307874937\n",
      "batch 399 Loss 0.3402004730185979 Acc 0.8560314897199346\n",
      "batch 499 Loss 0.42242989924607793 Acc 0.8507971429016202\n",
      "batch 599 Loss 0.2732303542741875 Acc 0.9098746424333707\n",
      "batch 699 Loss 0.4529459584142596 Acc 0.7824080939378985\n",
      "batch 799 Loss 0.49594157637959463 Acc 0.7604063587800296\n",
      "batch 899 Loss 0.4970557269238307 Acc 0.7638139093506174\n",
      "batch 999 Loss 0.4595912531480637 Acc 0.7923986009318743\n",
      "Epoch 1 Loss 0.45444820594933305 Acc 0.8100842030955445\n",
      "batch 99 Loss 0.418505442567995 Acc 0.8136996548299511\n",
      "batch 199 Loss 0.3943853280501295 Acc 0.8199785624285761\n",
      "batch 299 Loss 0.43692510075614893 Acc 0.7703377708183264\n",
      "batch 399 Loss 0.38378021339677415 Acc 0.7834066876977822\n",
      "batch 499 Loss 0.4309944325858098 Acc 0.7638025221120085\n",
      "batch 599 Loss 0.34487428022603434 Acc 0.8424142089631905\n",
      "batch 699 Loss 0.3980801640695521 Acc 0.7937392505145311\n",
      "batch 799 Loss 0.3492111803028189 Acc 0.842877842794181\n",
      "batch 899 Loss 0.3790039950195397 Acc 0.8263965665692665\n",
      "batch 999 Loss 0.3785507048251681 Acc 0.8012599231696125\n",
      "Epoch 2 Loss 0.4317316780936053 Acc 0.7639878613277098\n",
      "batch 99 Loss 0.39051003763430087 Acc 0.8241088366282845\n",
      "batch 199 Loss 0.3694437789519226 Acc 0.8396841574879463\n",
      "batch 299 Loss 0.33485009644374963 Acc 0.8551679972994125\n",
      "batch 399 Loss 0.3084684966600082 Acc 0.870471755959622\n",
      "batch 499 Loss 0.3205184743443529 Acc 0.8634838322764874\n",
      "batch 599 Loss 0.3248970934193276 Acc 0.8706927042916441\n",
      "batch 699 Loss 0.25751928257678264 Acc 0.8993717948711104\n",
      "batch 799 Loss 0.24563746133614775 Acc 0.8772497181154932\n",
      "batch 899 Loss 0.1789238176973645 Acc 0.9312795981829909\n",
      "batch 999 Loss 0.270079554795368 Acc 0.8722496715785294\n",
      "Epoch 3 Loss 0.27954002525579535 Acc 0.8641731606377374\n",
      "batch 99 Loss 0.2767632799777095 Acc 0.8750460602603877\n",
      "batch 199 Loss 0.2172898832242126 Acc 0.8926764476339034\n",
      "batch 299 Loss 0.3367863374942838 Acc 0.8724920098781105\n",
      "batch 399 Loss 0.46721467871966826 Acc 0.8216705284746566\n",
      "batch 499 Loss 0.3356375237020911 Acc 0.8845927303483605\n",
      "batch 599 Loss 0.3093806119311252 Acc 0.8786332009847536\n",
      "batch 699 Loss 0.25953618831106107 Acc 0.9158289261992962\n",
      "batch 799 Loss 0.22921682524500742 Acc 0.9330587214060178\n",
      "batch 899 Loss 0.16826682138628138 Acc 0.9508642255713265\n",
      "batch 999 Loss 0.20431713118152411 Acc 0.9381140474741424\n",
      "Epoch 4 Loss 0.19089616366301376 Acc 0.9336281871109952\n",
      "Final accuracy: 0.9065155807365439\n",
      "batch 99 Loss 0.7191192282533213 Acc 0.6029430896474641\n",
      "batch 199 Loss 0.635128793145532 Acc 0.6803636656506262\n",
      "batch 299 Loss 0.7353984962622943 Acc 0.6007848304914841\n",
      "batch 399 Loss 0.7817510201792884 Acc 0.5836698236591367\n",
      "batch 499 Loss 0.7459366709188815 Acc 0.6251587509683131\n",
      "batch 599 Loss 0.7346465951699586 Acc 0.6487389262195793\n",
      "batch 699 Loss 0.6237376407883867 Acc 0.692967401307674\n",
      "batch 799 Loss 0.7664669264944673 Acc 0.6067257026280545\n",
      "batch 899 Loss 0.7635949356531738 Acc 0.6111659556789557\n",
      "batch 999 Loss 0.7474536620583068 Acc 0.5834943509259538\n",
      "Epoch 0 Loss 0.7167581309993063 Acc 0.6095178998654932\n",
      "batch 99 Loss 0.6880131463123008 Acc 0.6528664091620257\n",
      "batch 199 Loss 0.7608673862092964 Acc 0.5819077621957314\n",
      "batch 299 Loss 0.7707017447130425 Acc 0.5454139914717157\n",
      "batch 399 Loss 0.7485287997531055 Acc 0.5369307784337847\n",
      "batch 499 Loss 0.7435942785219 Acc 0.5855586981155313\n",
      "batch 599 Loss 0.7990831068700314 Acc 0.5503199759077925\n",
      "batch 699 Loss 0.7795217283563887 Acc 0.5633718319541318\n",
      "batch 799 Loss 0.7898449398784474 Acc 0.6115366751945185\n",
      "batch 899 Loss 0.737975806730857 Acc 0.6133169743960822\n",
      "batch 999 Loss 0.7207498471755025 Acc 0.606013819274266\n",
      "Epoch 1 Loss 0.6835339663431593 Acc 0.601777071009052\n",
      "batch 99 Loss 0.7378391506715097 Acc 0.5916843805844463\n",
      "batch 199 Loss 0.6640605643510785 Acc 0.6614964035666622\n",
      "batch 299 Loss 0.759702604319787 Acc 0.6060796537899711\n",
      "batch 399 Loss 0.6969732948033253 Acc 0.6395117385680843\n",
      "batch 499 Loss 0.7653673022337394 Acc 0.6323230022972766\n",
      "batch 599 Loss 0.7577264166652498 Acc 0.6141563888216129\n",
      "batch 699 Loss 0.822218949268255 Acc 0.6208660498724701\n",
      "batch 799 Loss 0.7036786750566841 Acc 0.6480419600959765\n",
      "batch 899 Loss 0.6747413346201483 Acc 0.6655204248019579\n",
      "batch 999 Loss 0.7649054785611253 Acc 0.649247762114893\n",
      "Epoch 2 Loss 0.7451087329038797 Acc 0.6610359331927823\n",
      "batch 99 Loss 0.7229941568894536 Acc 0.6459790555795216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 199 Loss 0.6999153401126215 Acc 0.674025576187964\n",
      "batch 299 Loss 0.6964332185044196 Acc 0.6758463508950977\n",
      "batch 399 Loss 0.758182352767888 Acc 0.6615615799955862\n",
      "batch 499 Loss 0.7778918484959784 Acc 0.6399580689040285\n",
      "batch 599 Loss 0.6996220263830355 Acc 0.6789968686781622\n",
      "batch 699 Loss 0.7107403005297368 Acc 0.6310814491471803\n",
      "batch 799 Loss 0.7671936429976595 Acc 0.6275104773864335\n",
      "batch 899 Loss 0.7273004408059797 Acc 0.645121984496663\n",
      "batch 999 Loss 0.7632513690162444 Acc 0.6052939122024075\n",
      "Epoch 3 Loss 0.7517338800619747 Acc 0.6213502877212884\n",
      "batch 99 Loss 0.7839349181215559 Acc 0.56310697665866\n",
      "batch 199 Loss 0.7813894709688121 Acc 0.6323161086001202\n",
      "batch 299 Loss 0.7130465794738682 Acc 0.6413323299615559\n",
      "batch 399 Loss 0.8140676341472399 Acc 0.57265715870532\n",
      "batch 499 Loss 0.7771496098883882 Acc 0.6061744519901362\n",
      "batch 599 Loss 0.8622973208490164 Acc 0.6116288833476228\n",
      "batch 699 Loss 0.7747244015669684 Acc 0.6448590005762913\n",
      "batch 799 Loss 0.7756000849491566 Acc 0.6012436709235405\n",
      "batch 899 Loss 0.8459174470757879 Acc 0.5584860671771019\n",
      "batch 999 Loss 0.7714489992122466 Acc 0.6165245493063788\n",
      "Epoch 4 Loss 0.7434943976994253 Acc 0.6338178403676401\n",
      "Final accuracy: 0.5892351274787535\n",
      "batch 99 Loss 0.6894474487981107 Acc 0.561770819575142\n",
      "batch 199 Loss 0.5996007575422516 Acc 0.6744667338277814\n",
      "batch 299 Loss 0.5908249520976516 Acc 0.6791644628335712\n",
      "batch 399 Loss 0.5175160191062592 Acc 0.711160712019757\n",
      "batch 499 Loss 0.3904987722196057 Acc 0.8058903777154058\n",
      "batch 599 Loss 0.3257865608974793 Acc 0.8721820619917907\n",
      "batch 699 Loss 0.2679174430086233 Acc 0.8988130685454767\n",
      "batch 799 Loss 0.25441202422912257 Acc 0.913909971116209\n",
      "batch 899 Loss 0.2280151153229126 Acc 0.9249645120148665\n",
      "batch 999 Loss 0.25779475090656007 Acc 0.9251134535288686\n",
      "Epoch 0 Loss 0.1916642931859161 Acc 0.9513678309222728\n",
      "batch 99 Loss 0.1743510695121822 Acc 0.943822973828032\n",
      "batch 199 Loss 0.18917354358620436 Acc 0.9171949026208743\n",
      "batch 299 Loss 0.20814391413145206 Acc 0.9188347014830858\n",
      "batch 399 Loss 0.19488564855623944 Acc 0.9486806490615599\n",
      "batch 499 Loss 0.1797241604023443 Acc 0.9503246790399666\n",
      "batch 599 Loss 0.175196417596859 Acc 0.9250583934857116\n",
      "batch 699 Loss 0.11910175960288284 Acc 0.9600104684727492\n",
      "batch 799 Loss 0.1561271217728748 Acc 0.9526184534267388\n",
      "batch 899 Loss 0.14917883633966234 Acc 0.9360312067246795\n",
      "batch 999 Loss 0.15381709118168557 Acc 0.9572812881016027\n",
      "Epoch 1 Loss 0.2191210810841846 Acc 0.9217711118721151\n",
      "batch 99 Loss 0.1602173066778355 Acc 0.9436746911429975\n",
      "batch 199 Loss 0.19544283759097328 Acc 0.9298632044892928\n",
      "batch 299 Loss 0.18118486023296915 Acc 0.9193516157817049\n",
      "batch 399 Loss 0.19734922088155343 Acc 0.9256926640034485\n",
      "batch 499 Loss 0.15596156840404019 Acc 0.9492560878898041\n",
      "batch 599 Loss 0.20444215208331212 Acc 0.9213015116094239\n",
      "batch 699 Loss 0.148202607376385 Acc 0.9460701740442855\n",
      "batch 799 Loss 0.17120631687287727 Acc 0.9322025784845082\n",
      "batch 899 Loss 0.2021357475734102 Acc 0.9296468911662151\n",
      "batch 999 Loss 0.1403860012857502 Acc 0.9574360706754903\n",
      "Epoch 2 Loss 0.13134987624570962 Acc 0.9575845117768133\n",
      "batch 99 Loss 0.0924610137644833 Acc 0.9723679911940544\n",
      "batch 199 Loss 0.08826891045077259 Acc 0.9656188187189023\n",
      "batch 299 Loss 0.0537982282916622 Acc 0.9804512435394442\n",
      "batch 399 Loss 0.09091902651708554 Acc 0.9736716865275794\n",
      "batch 499 Loss 0.11798201408689907 Acc 0.9623222519541275\n",
      "batch 599 Loss 0.08183689534439009 Acc 0.9817334935312286\n",
      "batch 699 Loss 0.10412653083451802 Acc 0.9627010594096678\n",
      "batch 799 Loss 0.12378667633424897 Acc 0.9518797388944221\n",
      "batch 899 Loss 0.1138300186471423 Acc 0.9497745946130303\n",
      "batch 999 Loss 0.14879578200330976 Acc 0.9609502930933493\n",
      "Epoch 3 Loss 0.14600381316073266 Acc 0.9553161370444173\n",
      "batch 99 Loss 0.06972650889579292 Acc 0.9836442610252358\n",
      "batch 199 Loss 0.04456349951545108 Acc 0.9940132705698134\n",
      "batch 299 Loss 0.0946826836230193 Acc 0.9514533797903765\n",
      "batch 399 Loss 0.0550514690925959 Acc 0.978303256660191\n",
      "batch 499 Loss 0.046125309690247314 Acc 0.983628858303486\n",
      "batch 599 Loss 0.06237565115503292 Acc 0.9669045390566685\n",
      "batch 699 Loss 0.15836105973004808 Acc 0.9465992967003805\n",
      "batch 799 Loss 0.1680343323510534 Acc 0.9499771848262084\n",
      "batch 899 Loss 0.1775067837966008 Acc 0.947128888510994\n",
      "batch 999 Loss 0.15247678031171324 Acc 0.9550903151504376\n",
      "Epoch 4 Loss 0.1750930156037801 Acc 0.9324705350450782\n",
      "Final accuracy: 0.9376770538243626\n",
      "batch 99 Loss 0.6581766377035181 Acc 0.6202556937615921\n",
      "batch 199 Loss 0.5821078168166721 Acc 0.7017580324368006\n",
      "batch 299 Loss 0.4963571651085823 Acc 0.7658777073229903\n",
      "batch 399 Loss 0.48305171755603366 Acc 0.7387541468960949\n",
      "batch 499 Loss 0.4683307251349691 Acc 0.779024082710147\n",
      "batch 599 Loss 0.39487297064860954 Acc 0.8372560879856994\n",
      "batch 699 Loss 0.48013123464471114 Acc 0.7762126665374274\n",
      "batch 799 Loss 0.4956572384380092 Acc 0.7824664670748226\n",
      "batch 899 Loss 0.4813144012331922 Acc 0.7849378922715348\n",
      "batch 999 Loss 0.5230524117325195 Acc 0.7495346229810222\n",
      "Epoch 0 Loss 0.4730145953636125 Acc 0.7687463891890998\n",
      "batch 99 Loss 0.46140841865967336 Acc 0.7900010055178506\n",
      "batch 199 Loss 0.4452739252616138 Acc 0.8154551753317301\n",
      "batch 299 Loss 0.3962990937803958 Acc 0.8375532087225098\n",
      "batch 399 Loss 0.4831389046528296 Acc 0.7760203419805399\n",
      "batch 499 Loss 0.49284342934795017 Acc 0.7643751257143285\n",
      "batch 599 Loss 0.44635786420117235 Acc 0.7988055678513928\n",
      "batch 699 Loss 0.3453384826972407 Acc 0.8339275994758464\n",
      "batch 799 Loss 0.4998555890566328 Acc 0.7741984846391872\n",
      "batch 899 Loss 0.4243376323680154 Acc 0.8007465044437833\n",
      "batch 999 Loss 0.4187166900087565 Acc 0.8124088171910503\n",
      "Epoch 1 Loss 0.3232662762544258 Acc 0.8666307499850492\n",
      "batch 99 Loss 0.21888745827672174 Acc 0.9134285099531992\n",
      "batch 199 Loss 0.44455124371055826 Acc 0.8316613334343643\n",
      "batch 299 Loss 0.4947656811140376 Acc 0.7348394560964969\n",
      "batch 399 Loss 0.45271729996434124 Acc 0.7664432600619697\n",
      "batch 499 Loss 0.4947683483106127 Acc 0.713354681877551\n",
      "batch 599 Loss 0.5287594297927006 Acc 0.7620336440142476\n",
      "batch 699 Loss 0.4708882022820734 Acc 0.7979530992591731\n",
      "batch 799 Loss 0.527392277511509 Acc 0.7432488437961631\n",
      "batch 899 Loss 0.5573055221783002 Acc 0.7211253141787601\n",
      "batch 999 Loss 0.5305197562703491 Acc 0.7402218969495354\n",
      "Epoch 2 Loss 0.4746569875792207 Acc 0.7772907946154175\n",
      "batch 99 Loss 0.3885855341160053 Acc 0.8361754589647952\n",
      "batch 199 Loss 0.4824566125775077 Acc 0.76501262958115\n",
      "batch 299 Loss 0.4523661483941252 Acc 0.7739834720054675\n",
      "batch 399 Loss 0.45825626073474723 Acc 0.7788124691158455\n",
      "batch 499 Loss 0.3678019244704571 Acc 0.8431296902325511\n",
      "batch 599 Loss 0.47583616624089126 Acc 0.8111524783501473\n",
      "batch 699 Loss 0.39712303341686034 Acc 0.8632869766373304\n",
      "batch 799 Loss 0.33890899902269706 Acc 0.8816303175030719\n",
      "batch 899 Loss 0.32060618150329984 Acc 0.8589867485765506\n",
      "batch 999 Loss 0.2862252016627626 Acc 0.8844450076360652\n",
      "Epoch 3 Loss 0.2909840501247561 Acc 0.8825994922918871\n",
      "batch 99 Loss 0.31807349672605795 Acc 0.8585703107540704\n",
      "batch 199 Loss 0.5039718876975763 Acc 0.7756066994606339\n",
      "batch 299 Loss 0.38488944269171244 Acc 0.8570019395713424\n",
      "batch 399 Loss 0.3955924259474219 Acc 0.8254017859949946\n",
      "batch 499 Loss 0.4323598611466165 Acc 0.8445512556167132\n",
      "batch 599 Loss 0.4121232544928735 Acc 0.8560632554020121\n",
      "batch 699 Loss 0.48479266951104055 Acc 0.7741300159215739\n",
      "batch 799 Loss 0.5393141990408782 Acc 0.7495879474451508\n",
      "batch 899 Loss 0.4997365682573537 Acc 0.7720020174260102\n",
      "batch 999 Loss 0.5473004783923175 Acc 0.7254910228809098\n",
      "Epoch 4 Loss 0.5696143181035381 Acc 0.7023888555232466\n",
      "Final accuracy: 0.71671388101983\n",
      "batch 99 Loss 0.8893488221199941 Acc 0.5023560413095666\n",
      "batch 199 Loss 0.7925448552641916 Acc 0.5456658911526806\n",
      "batch 299 Loss 1.1262328999296187 Acc 0.4832763096592442\n",
      "batch 399 Loss 0.9783949645793653 Acc 0.5103203344707669\n",
      "batch 499 Loss 0.8819113743411853 Acc 0.5718312005557752\n",
      "batch 599 Loss 0.9171903431382449 Acc 0.5537858182693548\n",
      "batch 699 Loss 0.8260772940241496 Acc 0.5776222221427882\n",
      "batch 799 Loss 0.811050450621419 Acc 0.5665823822347036\n",
      "batch 899 Loss 0.814628194929063 Acc 0.5310821841096582\n",
      "batch 999 Loss 0.9365948423129583 Acc 0.5012141949227928\n",
      "Epoch 0 Loss 0.8661369851272395 Acc 0.5291673893982555\n",
      "batch 99 Loss 0.7722739322391898 Acc 0.6146503627437352\n",
      "batch 199 Loss 0.8325678926468176 Acc 0.5754196546161798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 299 Loss 0.9008175050489206 Acc 0.5497390711200434\n",
      "batch 399 Loss 0.8627725850154366 Acc 0.559488984875775\n",
      "batch 499 Loss 0.9166894551455351 Acc 0.5595192260346036\n",
      "batch 599 Loss 1.028829331423228 Acc 0.5702703455646485\n",
      "batch 699 Loss 1.0440773563626635 Acc 0.5473328896911931\n",
      "batch 799 Loss 0.9126141569428391 Acc 0.5481491533336174\n",
      "batch 899 Loss 0.8285546153720692 Acc 0.5406207673616358\n",
      "batch 999 Loss 0.7736159984814307 Acc 0.6094969992210771\n",
      "Epoch 1 Loss 0.9823598504946953 Acc 0.5678535451805692\n",
      "batch 99 Loss 0.8118138489707809 Acc 0.6611545175600805\n",
      "batch 199 Loss 0.7998729633489222 Acc 0.661327556623255\n",
      "batch 299 Loss 0.8926119748826398 Acc 0.6112268039877938\n",
      "batch 399 Loss 0.8353235447659232 Acc 0.6254092010784538\n",
      "batch 499 Loss 0.9008066595448931 Acc 0.6278441569982175\n",
      "batch 599 Loss 0.8039494470047975 Acc 0.6498422356753144\n",
      "batch 699 Loss 0.8113323431456906 Acc 0.6278228410493212\n",
      "batch 799 Loss 0.8526169295897501 Acc 0.6406471844063568\n",
      "batch 899 Loss 1.0964416553342637 Acc 0.5541106666037967\n",
      "batch 999 Loss 0.8768085263497826 Acc 0.6451291242841064\n",
      "Epoch 2 Loss 0.9800348517476808 Acc 0.5777288869914213\n",
      "batch 99 Loss 0.9019620118043103 Acc 0.551744209358202\n",
      "batch 199 Loss 0.8341040882022385 Acc 0.5943201173086676\n",
      "batch 299 Loss 0.8124833884850143 Acc 0.6141607746073021\n",
      "batch 399 Loss 0.8586082062273551 Acc 0.55424402747172\n",
      "batch 499 Loss 0.8856815405292157 Acc 0.5849553923818986\n",
      "batch 599 Loss 0.8200275476408038 Acc 0.6388779920408251\n",
      "batch 699 Loss 0.8739899488621196 Acc 0.625292760618182\n",
      "batch 799 Loss 0.7967394044322128 Acc 0.5875112948847828\n",
      "batch 899 Loss 0.9645958321253474 Acc 0.5086181660395406\n",
      "batch 999 Loss 0.9763468394380115 Acc 0.5268024195408721\n",
      "Epoch 3 Loss 0.920306406092634 Acc 0.5491545395150761\n",
      "batch 99 Loss 0.8664473488582619 Acc 0.6167004486308446\n",
      "batch 199 Loss 0.8389652895150791 Acc 0.5948349875988544\n",
      "batch 299 Loss 0.8265271861982563 Acc 0.5263596625112961\n",
      "batch 399 Loss 0.7409702326414843 Acc 0.6168358020580063\n",
      "batch 499 Loss 0.9540011222309197 Acc 0.5819739964970299\n",
      "batch 599 Loss 0.8351377682475895 Acc 0.5532127995508855\n",
      "batch 699 Loss 0.7928236331069073 Acc 0.5356957490963199\n",
      "batch 799 Loss 0.7040319264808239 Acc 0.6674970611381348\n",
      "batch 899 Loss 0.6886078599110658 Acc 0.6997030485622617\n",
      "batch 999 Loss 0.8653956005321028 Acc 0.5868234994318657\n",
      "Epoch 4 Loss 0.7767556857402357 Acc 0.6264220422193175\n",
      "Final accuracy: 0.49008498583569404\n",
      "batch 99 Loss 0.6786621899919614 Acc 0.5755440399104556\n",
      "batch 199 Loss 0.5865907778632872 Acc 0.6966417854199132\n",
      "batch 299 Loss 0.5099615330406775 Acc 0.765156524030509\n",
      "batch 399 Loss 0.4500073442573537 Acc 0.8130141883213752\n",
      "batch 499 Loss 0.408523662166019 Acc 0.834419163458043\n",
      "batch 599 Loss 0.35568670752945264 Acc 0.8737333393713914\n",
      "batch 699 Loss 0.3778413257581658 Acc 0.8449435794898331\n",
      "batch 799 Loss 0.39718484064569465 Acc 0.8207280403878768\n",
      "batch 899 Loss 0.37971082386530947 Acc 0.8251443136628076\n",
      "batch 999 Loss 0.4496181146281964 Acc 0.8043892085325717\n",
      "Epoch 0 Loss 0.37794456404724464 Acc 0.8533737977003785\n",
      "batch 99 Loss 0.34080234332795023 Acc 0.8793598298405749\n",
      "batch 199 Loss 0.25090940217443747 Acc 0.9146230983845589\n",
      "batch 299 Loss 0.23232672450216726 Acc 0.9173474196993106\n",
      "batch 399 Loss 0.19590229471133672 Acc 0.9356059839255576\n",
      "batch 499 Loss 0.22714010963159734 Acc 0.9307349248708026\n",
      "batch 599 Loss 0.21670596872620432 Acc 0.9253410962040688\n",
      "batch 699 Loss 0.32242129018091226 Acc 0.8537298848025359\n",
      "batch 799 Loss 0.2960091299768293 Acc 0.8803538649781664\n",
      "batch 899 Loss 0.24903631353080818 Acc 0.8979968979527235\n",
      "batch 999 Loss 0.2390302581519359 Acc 0.9240403677881444\n",
      "Epoch 1 Loss 0.20338486533668212 Acc 0.9276980325905597\n",
      "batch 99 Loss 0.22586457320099118 Acc 0.9149683585377238\n",
      "batch 199 Loss 0.1744370411171826 Acc 0.9421417411174101\n",
      "batch 299 Loss 0.1527894861433524 Acc 0.9445804262945429\n",
      "batch 399 Loss 0.15443639022228686 Acc 0.9344737916623378\n",
      "batch 499 Loss 0.15275821976127452 Acc 0.9452627858417095\n",
      "batch 599 Loss 0.14365190307943992 Acc 0.9590208977024366\n",
      "batch 699 Loss 0.1661964269135494 Acc 0.9231569539308361\n",
      "batch 799 Loss 0.12973685417149472 Acc 0.957135441137672\n",
      "batch 899 Loss 0.15313047755583306 Acc 0.9393106864573515\n",
      "batch 999 Loss 0.14110660745967285 Acc 0.9493695936930111\n",
      "Epoch 2 Loss 0.18736957777836263 Acc 0.9397903188113738\n",
      "batch 99 Loss 0.15359092012000897 Acc 0.9383915029597945\n",
      "batch 199 Loss 0.12725494410011778 Acc 0.9426200384951409\n",
      "batch 299 Loss 0.1268483286333224 Acc 0.9444072116177239\n",
      "batch 399 Loss 0.1374603884960777 Acc 0.9366304740840095\n",
      "batch 499 Loss 0.15415043575819812 Acc 0.9323475835178414\n",
      "batch 599 Loss 0.1160130975755877 Acc 0.9526556416326514\n",
      "batch 699 Loss 0.09767832306769256 Acc 0.9647902977665401\n",
      "batch 799 Loss 0.09924975626799615 Acc 0.964669422902797\n",
      "batch 899 Loss 0.0863893767479845 Acc 0.9761810789446347\n",
      "batch 999 Loss 0.07544833972765845 Acc 0.9847904782758184\n",
      "Epoch 3 Loss 0.1608003655944501 Acc 0.9630582581877931\n",
      "batch 99 Loss 0.11101221078950307 Acc 0.97469306763026\n",
      "batch 199 Loss 0.05980737824759359 Acc 0.990736844294261\n",
      "batch 299 Loss 0.04443530691154635 Acc 0.9870994849304501\n",
      "batch 399 Loss 0.034289172137774236 Acc 0.9895819822406905\n",
      "batch 499 Loss 0.15643428029117928 Acc 0.944264423081593\n",
      "batch 599 Loss 0.13018408815200652 Acc 0.9422470212039608\n",
      "batch 699 Loss 0.11009855143829403 Acc 0.958946367917246\n",
      "batch 799 Loss 0.07939353481261317 Acc 0.978482016647296\n",
      "batch 899 Loss 0.15781704639169664 Acc 0.9308357740464989\n",
      "batch 999 Loss 0.11134471179876208 Acc 0.9553682135345354\n",
      "Epoch 4 Loss 0.10913021741059306 Acc 0.9595955580747259\n",
      "Final accuracy: 0.9093484419263456\n",
      "batch 99 Loss 0.7014734619534068 Acc 0.5953367210338659\n",
      "batch 199 Loss 0.6853807822378224 Acc 0.6125966691549297\n",
      "batch 299 Loss 0.6593224801746884 Acc 0.6324273784116016\n",
      "batch 399 Loss 0.6898404694925502 Acc 0.5694090017497319\n",
      "batch 499 Loss 0.6623047229775934 Acc 0.5993426103215411\n",
      "batch 599 Loss 0.6278919980832197 Acc 0.6627929423504887\n",
      "batch 699 Loss 0.5932863395338103 Acc 0.7192855431078102\n",
      "batch 799 Loss 0.6610008125164045 Acc 0.6421018298095713\n",
      "batch 899 Loss 0.6407754422001285 Acc 0.6582374004151257\n",
      "batch 999 Loss 0.6653890385927884 Acc 0.6462230840461065\n",
      "Epoch 0 Loss 0.6085513502980613 Acc 0.7103160889564192\n",
      "batch 99 Loss 0.5117489750542866 Acc 0.7892234606559899\n",
      "batch 199 Loss 0.507389808247701 Acc 0.7400130329630183\n",
      "batch 299 Loss 0.5936870602571298 Acc 0.7054655049189259\n",
      "batch 399 Loss 0.5976701862993342 Acc 0.6539877549472636\n",
      "batch 499 Loss 0.5363458593915006 Acc 0.7143153505847265\n",
      "batch 599 Loss 0.5212726369974058 Acc 0.7785809602723524\n",
      "batch 699 Loss 0.46563126305425295 Acc 0.8068090058603299\n",
      "batch 799 Loss 0.4832647316037348 Acc 0.7810888104153265\n",
      "batch 899 Loss 0.5027301939044313 Acc 0.7621545597342159\n",
      "batch 999 Loss 0.5565713579445218 Acc 0.725830109276703\n",
      "Epoch 1 Loss 0.590929730561381 Acc 0.6759202991592425\n",
      "batch 99 Loss 0.48552819862686974 Acc 0.7912256088935682\n",
      "batch 199 Loss 0.4614127743066272 Acc 0.7817542192408389\n",
      "batch 299 Loss 0.4852112951112732 Acc 0.752754232882573\n",
      "batch 399 Loss 0.4231235398917458 Acc 0.8248477314223002\n",
      "batch 499 Loss 0.41957671273131053 Acc 0.8348751504743255\n",
      "batch 599 Loss 0.5215694710894223 Acc 0.7715536186116415\n",
      "batch 699 Loss 0.5261740702511216 Acc 0.7427395781252049\n",
      "batch 799 Loss 0.6037761211357583 Acc 0.7044228004688142\n",
      "batch 899 Loss 0.5569228536110312 Acc 0.7284178597904973\n",
      "batch 999 Loss 0.44156895602489193 Acc 0.8071501277615472\n",
      "Epoch 2 Loss 0.34365135919936457 Acc 0.8721371041653497\n",
      "batch 99 Loss 0.3105794067563637 Acc 0.8920479473274385\n",
      "batch 199 Loss 0.4016386250179901 Acc 0.8422312347578164\n",
      "batch 299 Loss 0.3482653506604456 Acc 0.8823866880394512\n",
      "batch 399 Loss 0.2798153842208299 Acc 0.8952738066312345\n",
      "batch 499 Loss 0.4151092649436466 Acc 0.7998064465741325\n",
      "batch 599 Loss 0.5437698488390759 Acc 0.7230588089980218\n",
      "batch 699 Loss 0.6019337273206644 Acc 0.6855480496860498\n",
      "batch 799 Loss 0.6057549441140438 Acc 0.6346875531629453\n",
      "batch 899 Loss 0.5911446610138112 Acc 0.621366422532834\n",
      "batch 999 Loss 0.5576220544493984 Acc 0.7133164490629994\n",
      "Epoch 3 Loss 0.614493384656646 Acc 0.6595318143519305\n",
      "batch 99 Loss 0.6175194524703019 Acc 0.6386312580425366\n",
      "batch 199 Loss 0.5966311936479586 Acc 0.7187557701363216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 299 Loss 0.6040048766161472 Acc 0.7680341201036825\n",
      "batch 399 Loss 0.6189196136391321 Acc 0.6903981481376905\n",
      "batch 499 Loss 0.5558889227882851 Acc 0.7185116108051592\n",
      "batch 599 Loss 0.48979867109341274 Acc 0.7728630732856252\n",
      "batch 699 Loss 0.51962734369642 Acc 0.7667221452136953\n",
      "batch 799 Loss 0.5680741126008407 Acc 0.7141250021154685\n",
      "batch 899 Loss 0.5183742205654934 Acc 0.7022111639388284\n",
      "batch 999 Loss 0.4951565644501967 Acc 0.752591685048371\n",
      "Epoch 4 Loss 0.5599841441388069 Acc 0.7004777086213305\n",
      "Final accuracy: 0.6458923512747875\n",
      "batch 99 Loss 1.2779178681445218 Acc 0.5839997188887098\n",
      "batch 199 Loss 1.1338655559852868 Acc 0.5400665633809886\n",
      "batch 299 Loss 1.467515512209052 Acc 0.4643499134822689\n",
      "batch 399 Loss 1.2331155568557042 Acc 0.49232586262044276\n",
      "batch 499 Loss 1.8945450139946631 Acc 0.486843036788335\n",
      "batch 599 Loss 2.127750557751528 Acc 0.4912568170269294\n",
      "batch 699 Loss 2.10154731930898 Acc 0.4974658030342793\n",
      "batch 799 Loss 1.748054223649093 Acc 0.47944006080080187\n",
      "batch 899 Loss 1.2771960422992394 Acc 0.5394816039122682\n",
      "batch 999 Loss 1.7087124804358023 Acc 0.5334295734235303\n",
      "Epoch 0 Loss 1.4842644067929087 Acc 0.5405787784907436\n",
      "batch 99 Loss 1.2034237918979587 Acc 0.4967879785943476\n",
      "batch 199 Loss 1.3495436000920946 Acc 0.5275204255563055\n",
      "batch 299 Loss 1.3221673152241107 Acc 0.4846197439351392\n",
      "batch 399 Loss 1.4254476015456767 Acc 0.4355014908750151\n",
      "batch 499 Loss 1.3760744843265504 Acc 0.4998935281111642\n",
      "batch 599 Loss 1.2510279525629355 Acc 0.5272554723567727\n",
      "batch 699 Loss 1.1206586121341846 Acc 0.49314036273263917\n",
      "batch 799 Loss 1.4724531100119098 Acc 0.5181597507610296\n",
      "batch 899 Loss 1.380113508696739 Acc 0.4956430062882146\n",
      "batch 999 Loss 1.496511302518116 Acc 0.4992668914113183\n",
      "Epoch 1 Loss 1.726855925697316 Acc 0.4967703031788895\n",
      "batch 99 Loss 1.2577525792864561 Acc 0.5887481845019276\n",
      "batch 199 Loss 1.3598293782979542 Acc 0.5639161354000115\n",
      "batch 299 Loss 1.7710312006081659 Acc 0.4843529821747167\n",
      "batch 399 Loss 1.5562761812041097 Acc 0.44426393861839714\n",
      "batch 499 Loss 2.024579112669948 Acc 0.5002674230519699\n",
      "batch 599 Loss 1.8682750340963086 Acc 0.5628650677457492\n",
      "batch 699 Loss 2.0601928130702705 Acc 0.561149607027752\n",
      "batch 799 Loss 1.6013048892362982 Acc 0.5221719052073818\n",
      "batch 899 Loss 1.5626940066726367 Acc 0.5354921054147852\n",
      "batch 999 Loss 1.330874692793525 Acc 0.5492831263922728\n",
      "Epoch 2 Loss 1.2541705879542333 Acc 0.5644148845193352\n",
      "batch 99 Loss 1.698611178118124 Acc 0.46394886155520965\n",
      "batch 199 Loss 1.976475244552553 Acc 0.5184291626214909\n",
      "batch 299 Loss 1.5655203340842951 Acc 0.4962279416215771\n",
      "batch 399 Loss 1.7063656717202442 Acc 0.4894588795585435\n",
      "batch 499 Loss 1.366333814874837 Acc 0.517233534502558\n",
      "batch 599 Loss 1.44990140708104 Acc 0.5099852657501074\n",
      "batch 699 Loss 1.2220035846265398 Acc 0.4861530543236095\n",
      "batch 799 Loss 1.1988956280217271 Acc 0.5720849925564658\n",
      "batch 899 Loss 1.2869879681352743 Acc 0.5007591947914127\n",
      "batch 999 Loss 1.2312851644176215 Acc 0.5598041642612666\n",
      "Epoch 3 Loss 1.291413409340264 Acc 0.5192006762795109\n",
      "batch 99 Loss 1.1584702305511405 Acc 0.5594759001588869\n",
      "batch 199 Loss 1.1628062913230373 Acc 0.5487053216829758\n",
      "batch 299 Loss 1.1157037126976919 Acc 0.4785671742116706\n",
      "batch 399 Loss 1.1214612828499353 Acc 0.465430067320534\n",
      "batch 499 Loss 1.0862337089007172 Acc 0.5254039075770335\n",
      "batch 599 Loss 1.1862525908621118 Acc 0.5044076495159546\n",
      "batch 699 Loss 1.4396990325543093 Acc 0.5670811779656135\n",
      "batch 799 Loss 1.3459049046973215 Acc 0.5481554590816843\n",
      "batch 899 Loss 1.3696828405116588 Acc 0.5287959063323927\n",
      "batch 999 Loss 1.1635635291812059 Acc 0.5479893091659712\n",
      "Epoch 4 Loss 1.18743651293347 Acc 0.5415128541031079\n",
      "Final accuracy: 0.4730878186968839\n",
      "batch 99 Loss 0.6663812147843202 Acc 0.6272540181699586\n",
      "batch 199 Loss 0.5757425484962293 Acc 0.6915727596578668\n",
      "batch 299 Loss 0.5900389361589617 Acc 0.656774325410073\n",
      "batch 399 Loss 0.5380126089571556 Acc 0.7003758655151866\n",
      "batch 499 Loss 0.4357966395268697 Acc 0.8028224558277851\n",
      "batch 599 Loss 0.48825197190266034 Acc 0.767391620032782\n",
      "batch 699 Loss 0.4414703865323724 Acc 0.8123570585608025\n",
      "batch 799 Loss 0.43836085483685155 Acc 0.8038517760110362\n",
      "batch 899 Loss 0.4594978073278649 Acc 0.8210010460564432\n",
      "batch 999 Loss 0.37718245130201294 Acc 0.8534015356555155\n",
      "Epoch 0 Loss 0.31942462948124284 Acc 0.8658261254253207\n",
      "batch 99 Loss 0.340417020479268 Acc 0.8873914616139852\n",
      "batch 199 Loss 0.39916499491883606 Acc 0.847991683135651\n",
      "batch 299 Loss 0.3798088691604359 Acc 0.833381239130795\n",
      "batch 399 Loss 0.3576114474179247 Acc 0.8368686336700223\n",
      "batch 499 Loss 0.2596230733097227 Acc 0.8931694464299827\n",
      "batch 599 Loss 0.2912547608272012 Acc 0.9075352934721465\n",
      "batch 699 Loss 0.2546305936749667 Acc 0.9060315605326598\n",
      "batch 799 Loss 0.22922381889290483 Acc 0.9018386293082589\n",
      "batch 899 Loss 0.2629025534365108 Acc 0.8882168883378285\n",
      "batch 999 Loss 0.3589398510175055 Acc 0.8633903763494554\n",
      "Epoch 1 Loss 0.34957394000933795 Acc 0.8572864147876916\n",
      "batch 99 Loss 0.3295145510954036 Acc 0.8634245298438765\n",
      "batch 199 Loss 0.24227744802485965 Acc 0.9093522005858059\n",
      "batch 299 Loss 0.17015235683219568 Acc 0.9438069518895051\n",
      "batch 399 Loss 0.19707979328617783 Acc 0.9184907307088097\n",
      "batch 499 Loss 0.23827869691427025 Acc 0.9196977784502759\n",
      "batch 599 Loss 0.23931649731111837 Acc 0.9092046630614488\n",
      "batch 699 Loss 0.175721834729077 Acc 0.9436084515497806\n",
      "batch 799 Loss 0.23383998502254508 Acc 0.9015834679772136\n",
      "batch 899 Loss 0.2218406913925575 Acc 0.9277476642500924\n",
      "batch 999 Loss 0.23418351487800187 Acc 0.915242282348286\n",
      "Epoch 2 Loss 0.17654859893362135 Acc 0.9280562452770937\n",
      "batch 99 Loss 0.15485478303925373 Acc 0.9425988116116881\n",
      "batch 199 Loss 0.09699757744320922 Acc 0.9648618303316413\n",
      "batch 299 Loss 0.28564934589200475 Acc 0.88612408346628\n",
      "batch 399 Loss 0.22074406522569268 Acc 0.9179362691924984\n",
      "batch 499 Loss 0.19733295473007048 Acc 0.936596482533701\n",
      "batch 599 Loss 0.2581988547090606 Acc 0.9057927575262293\n",
      "batch 699 Loss 0.20678467657005822 Acc 0.9276547730095015\n",
      "batch 799 Loss 0.1642626006010311 Acc 0.9399393303212963\n",
      "batch 899 Loss 0.12606184672783508 Acc 0.9601642693606454\n",
      "batch 999 Loss 0.08549788998441853 Acc 0.9767767579957618\n",
      "Epoch 3 Loss 0.11929882078976356 Acc 0.9699510220093799\n",
      "batch 99 Loss 0.15576867884812326 Acc 0.9486317500644588\n",
      "batch 199 Loss 0.2471272906549817 Acc 0.8827371965905814\n",
      "batch 299 Loss 0.21555102438042545 Acc 0.9057601657364852\n",
      "batch 399 Loss 0.22849701117970256 Acc 0.9139262695974044\n",
      "batch 499 Loss 0.13084021329020876 Acc 0.9515688401553467\n",
      "batch 599 Loss 0.21645743413397048 Acc 0.926898612991922\n",
      "batch 699 Loss 0.20252330303729776 Acc 0.9439194901738099\n",
      "batch 799 Loss 0.2913608119493669 Acc 0.8955070800507844\n",
      "batch 899 Loss 0.25194910945157645 Acc 0.902497835086288\n",
      "batch 999 Loss 0.1780256079028193 Acc 0.9410254870552248\n",
      "Epoch 4 Loss 0.13968400753687918 Acc 0.9545430361246349\n",
      "Final accuracy: 0.943342776203966\n",
      "batch 99 Loss 0.8359717048804197 Acc 0.523752951312302\n",
      "batch 199 Loss 0.8154713227168406 Acc 0.5346591293891854\n",
      "batch 299 Loss 0.8915650594144321 Acc 0.4966878908429636\n",
      "batch 399 Loss 0.8355172500738313 Acc 0.5125947905373088\n",
      "batch 499 Loss 0.8506202156696002 Acc 0.5359850185881057\n",
      "batch 599 Loss 0.8511326936443656 Acc 0.45238139821215084\n",
      "batch 699 Loss 0.8377677261553987 Acc 0.4922647008399834\n",
      "batch 799 Loss 0.8186816465561526 Acc 0.5250526221973045\n",
      "batch 899 Loss 0.8353416069284526 Acc 0.535681691972805\n",
      "batch 999 Loss 0.8820531715423614 Acc 0.5556621843005621\n",
      "Epoch 0 Loss 0.8474908341890344 Acc 0.5042859427463873\n",
      "batch 99 Loss 0.7967519984970314 Acc 0.5232911126457882\n",
      "batch 199 Loss 0.7836320111519288 Acc 0.5391694213596859\n",
      "batch 299 Loss 0.809160079191144 Acc 0.5288143883797717\n",
      "batch 399 Loss 0.7990931636721169 Acc 0.5641378134982671\n",
      "batch 499 Loss 0.8074125154687375 Acc 0.5269428650711138\n",
      "batch 599 Loss 0.8406387888646776 Acc 0.49448482639082414\n",
      "batch 699 Loss 0.8112734128674388 Acc 0.5377181653782689\n",
      "batch 799 Loss 0.8391879801371523 Acc 0.4997170040306138\n",
      "batch 899 Loss 0.7895798941726475 Acc 0.5335615695909637\n",
      "batch 999 Loss 0.8373009083862124 Acc 0.48297481624185407\n",
      "Epoch 1 Loss 0.7915936570928697 Acc 0.541975302637163\n",
      "batch 99 Loss 0.8005077178196938 Acc 0.5736089008023096\n",
      "batch 199 Loss 0.7785192637967421 Acc 0.5627995191865114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 299 Loss 0.7669031590226141 Acc 0.5912613378502917\n",
      "batch 399 Loss 0.820261545155693 Acc 0.5561833136926619\n",
      "batch 499 Loss 0.7986800935409806 Acc 0.509223803225904\n",
      "batch 599 Loss 0.8281327616727683 Acc 0.47840913768819154\n",
      "batch 699 Loss 0.7992261125375523 Acc 0.532117893556792\n",
      "batch 799 Loss 0.7934839433999653 Acc 0.5139918123699265\n",
      "batch 899 Loss 0.8184559222478046 Acc 0.5076895512938444\n",
      "batch 999 Loss 0.7982031589518148 Acc 0.5085114296283384\n",
      "Epoch 2 Loss 0.7851258012969545 Acc 0.5090109180038495\n",
      "batch 99 Loss 0.844658489891707 Acc 0.5397939698252321\n",
      "batch 199 Loss 0.8730173143869551 Acc 0.5254573879215967\n",
      "batch 299 Loss 0.8281851204738335 Acc 0.5401213604288934\n",
      "batch 399 Loss 0.826237049474842 Acc 0.565071548073966\n",
      "batch 499 Loss 0.8917416495199503 Acc 0.49651947913879424\n",
      "batch 599 Loss 0.82704617404733 Acc 0.5007636068998558\n",
      "batch 699 Loss 0.737016414903358 Acc 0.604657234659229\n",
      "batch 799 Loss 0.7510710688043167 Acc 0.5895789910659714\n",
      "batch 899 Loss 0.7886255790632327 Acc 0.5112791573276547\n",
      "batch 999 Loss 0.778355842806131 Acc 0.5468836491667368\n",
      "Epoch 3 Loss 0.7785902821249672 Acc 0.5155383755863225\n",
      "batch 99 Loss 0.8157067748027816 Acc 0.5439208618003566\n",
      "batch 199 Loss 0.7791287394845477 Acc 0.5779996998256353\n",
      "batch 299 Loss 0.8432581082160509 Acc 0.5135649330847206\n",
      "batch 399 Loss 0.7925962938213897 Acc 0.4933274749036361\n",
      "batch 499 Loss 0.8151372884971285 Acc 0.5259331965275185\n",
      "batch 599 Loss 0.863741189960145 Acc 0.4907262812675435\n",
      "batch 699 Loss 0.824181468773419 Acc 0.5020768880242452\n",
      "batch 799 Loss 0.7963186665240435 Acc 0.5480306002043848\n",
      "batch 899 Loss 0.7317254993164403 Acc 0.6018077463040927\n",
      "batch 999 Loss 0.8513790092785218 Acc 0.5188996320638555\n",
      "Epoch 4 Loss 0.7749392332464197 Acc 0.5557976224940953\n",
      "Final accuracy: 0.5155807365439093\n",
      "batch 99 Loss 1.9168030530348992 Acc 0.520653949599519\n",
      "batch 199 Loss 1.8136527721446265 Acc 0.518908384078404\n",
      "batch 299 Loss 1.8006544788463588 Acc 0.5118114667610769\n",
      "batch 399 Loss 2.0053981420688345 Acc 0.5769833904628353\n",
      "batch 499 Loss 2.865114465521092 Acc 0.5112470273521151\n",
      "batch 599 Loss 1.8875734714867145 Acc 0.548531304273112\n",
      "batch 699 Loss 2.695498562310857 Acc 0.5619145412819189\n",
      "batch 799 Loss 2.0487626030671544 Acc 0.5336722925753237\n",
      "batch 899 Loss 2.6887135176660677 Acc 0.5293837859216515\n",
      "batch 999 Loss 1.9894415177520413 Acc 0.49953267138523666\n",
      "Epoch 0 Loss 2.0971346305747924 Acc 0.5308547966574189\n",
      "batch 99 Loss 2.1303646181604887 Acc 0.5138033010234797\n",
      "batch 199 Loss 1.967135182069046 Acc 0.5340863313499324\n",
      "batch 299 Loss 1.6970323144940012 Acc 0.5785309158377471\n",
      "batch 399 Loss 3.245809747935133 Acc 0.5206987202134592\n",
      "batch 499 Loss 3.3000878718507978 Acc 0.5903083527694936\n",
      "batch 599 Loss 2.411713651951548 Acc 0.5421584569988949\n",
      "batch 699 Loss 1.8783417508798306 Acc 0.5244460102388155\n",
      "batch 799 Loss 1.8246057896712458 Acc 0.516695939049571\n",
      "batch 899 Loss 2.110825980742449 Acc 0.5048135285692984\n",
      "batch 999 Loss 2.6880866188150403 Acc 0.5661205527018441\n",
      "Epoch 1 Loss 2.386192608301824 Acc 0.5083676164339518\n",
      "batch 99 Loss 2.453095592352355 Acc 0.429128706638522\n",
      "batch 199 Loss 2.132527632173226 Acc 0.47164944660082014\n",
      "batch 299 Loss 1.944726520834947 Acc 0.46300047705742914\n",
      "batch 399 Loss 2.9884689511770754 Acc 0.48632942591716266\n",
      "batch 499 Loss 2.0651326235589273 Acc 0.47598735903781636\n",
      "batch 599 Loss 1.653455310392936 Acc 0.5043405826407292\n",
      "batch 699 Loss 1.7604348064333386 Acc 0.5258359671385964\n",
      "batch 799 Loss 2.439485610803063 Acc 0.5799753690120529\n",
      "batch 899 Loss 2.3400623262840274 Acc 0.5534011321456996\n",
      "batch 999 Loss 2.686170998071408 Acc 0.49580314639128403\n",
      "Epoch 2 Loss 2.7733910925536147 Acc 0.49178044509974556\n",
      "batch 99 Loss 2.917868603818909 Acc 0.4478632058458522\n",
      "batch 199 Loss 2.2972111017229455 Acc 0.46976719584623894\n",
      "batch 299 Loss 2.0395248509345207 Acc 0.5410747820053156\n",
      "batch 399 Loss 1.957956328323806 Acc 0.5731538901982912\n",
      "batch 499 Loss 1.8095100619907956 Acc 0.5392041950242628\n",
      "batch 599 Loss 1.6035832805398107 Acc 0.48517250594669636\n",
      "batch 699 Loss 1.7289346592303452 Acc 0.49378339683323147\n",
      "batch 799 Loss 1.7594309749932662 Acc 0.46558906478944145\n",
      "batch 899 Loss 2.5264240330332175 Acc 0.502084163883128\n",
      "batch 999 Loss 2.407232224560262 Acc 0.5406154722657974\n",
      "Epoch 3 Loss 2.0996321557869133 Acc 0.5359558629904265\n",
      "batch 99 Loss 2.570161190796889 Acc 0.5276519086129894\n",
      "batch 199 Loss 2.5324625856346703 Acc 0.5194578520952284\n",
      "batch 299 Loss 2.68227869686053 Acc 0.5202628235941299\n",
      "batch 399 Loss 2.3508297198491546 Acc 0.5024710912516941\n",
      "batch 499 Loss 1.75060700246293 Acc 0.4760389479060373\n",
      "batch 599 Loss 1.9604636763587027 Acc 0.49831081732117666\n",
      "batch 699 Loss 2.203460089856896 Acc 0.4978362891022055\n",
      "batch 799 Loss 3.3264610269430017 Acc 0.559948128029125\n",
      "batch 899 Loss 3.059709168499929 Acc 0.49020373729466066\n",
      "batch 999 Loss 2.4310645637284893 Acc 0.49173615588704667\n",
      "Epoch 4 Loss 1.883591853741197 Acc 0.521771562567721\n",
      "Final accuracy: 0.5155807365439093\n",
      "batch 99 Loss 0.6750579618800735 Acc 0.6305612655806774\n",
      "batch 199 Loss 0.7058901769935705 Acc 0.6242678735975541\n",
      "batch 299 Loss 0.6970601027250013 Acc 0.6390035739523077\n",
      "batch 399 Loss 0.6895120881000736 Acc 0.6006198381688291\n",
      "batch 499 Loss 0.622355223537545 Acc 0.6636531318555661\n",
      "batch 599 Loss 0.5950665557535169 Acc 0.7079680895936001\n",
      "batch 699 Loss 0.5652233990802621 Acc 0.7377149531844214\n",
      "batch 799 Loss 0.5024474303994935 Acc 0.7626993439994586\n",
      "batch 899 Loss 0.47479456896547945 Acc 0.779657183281156\n",
      "batch 999 Loss 0.425233008891311 Acc 0.8161682727138935\n",
      "Epoch 0 Loss 0.3786477316531772 Acc 0.8477497537771701\n",
      "batch 99 Loss 0.41048200874701524 Acc 0.7959734257501053\n",
      "batch 199 Loss 0.42157316376118514 Acc 0.7882534689495047\n",
      "batch 299 Loss 0.37433091382047146 Acc 0.8255060196911791\n",
      "batch 399 Loss 0.4684918998484012 Acc 0.7690334601265749\n",
      "batch 499 Loss 0.40179185310599247 Acc 0.8043899281386736\n",
      "batch 599 Loss 0.5472206390517866 Acc 0.7114359162893517\n",
      "batch 699 Loss 0.5863795165310032 Acc 0.6760500796771151\n",
      "batch 799 Loss 0.5886889974576656 Acc 0.6800477120585428\n",
      "batch 899 Loss 0.6035291904295949 Acc 0.6584682224049938\n",
      "batch 999 Loss 0.6146268913853584 Acc 0.6248405856592621\n",
      "Epoch 1 Loss 0.6273747973872528 Acc 0.6483835034348153\n",
      "batch 99 Loss 0.6562176183495261 Acc 0.6359409473202912\n",
      "batch 199 Loss 0.6097527043769503 Acc 0.6561179950961671\n",
      "batch 299 Loss 0.6121246862921834 Acc 0.6270086052851026\n",
      "batch 399 Loss 0.5769926655619658 Acc 0.665157709551335\n",
      "batch 499 Loss 0.5903156977129285 Acc 0.6594569531975834\n",
      "batch 599 Loss 0.5607598286662633 Acc 0.6842917464574962\n",
      "batch 699 Loss 0.6108113630275717 Acc 0.6542834543705852\n",
      "batch 799 Loss 0.5585926844660999 Acc 0.7027065063161129\n",
      "batch 899 Loss 0.5421392743764648 Acc 0.7236298357334436\n",
      "batch 999 Loss 0.5852488220006321 Acc 0.7050551398812592\n",
      "Epoch 2 Loss 0.5687331544255299 Acc 0.7105962797326517\n",
      "batch 99 Loss 0.5549102237981841 Acc 0.733301461074591\n",
      "batch 199 Loss 0.49752078999145494 Acc 0.7914410359999118\n",
      "batch 299 Loss 0.4782229709842945 Acc 0.7656532700531339\n",
      "batch 399 Loss 0.56459750884949 Acc 0.7184465337737277\n",
      "batch 499 Loss 0.5594704655221611 Acc 0.7162518033695796\n",
      "batch 599 Loss 0.552365088536393 Acc 0.7011625283115102\n",
      "batch 699 Loss 0.5778316658911039 Acc 0.7214831431584182\n",
      "batch 799 Loss 0.5856403132115916 Acc 0.7309753241468716\n",
      "batch 899 Loss 0.6184345142689031 Acc 0.7460935124693321\n",
      "batch 999 Loss 0.5958683276807022 Acc 0.6988098742766257\n",
      "Epoch 3 Loss 0.5864986416961787 Acc 0.6928098030453063\n",
      "batch 99 Loss 0.5450729410367774 Acc 0.7022254063790005\n",
      "batch 199 Loss 0.4989580173197399 Acc 0.726040296204865\n",
      "batch 299 Loss 0.5237532471136157 Acc 0.7002696154290625\n",
      "batch 399 Loss 0.516726291085505 Acc 0.7037010265054722\n",
      "batch 499 Loss 0.5247772250537667 Acc 0.7161228630086027\n",
      "batch 599 Loss 0.5773397417880947 Acc 0.6734848352036816\n",
      "batch 699 Loss 0.5764719143328123 Acc 0.6734316590519173\n",
      "batch 799 Loss 0.5462154535966781 Acc 0.7096621068608305\n",
      "batch 899 Loss 0.562472264466263 Acc 0.67788799883755\n",
      "batch 999 Loss 0.48217207324787936 Acc 0.7330000687820156\n",
      "Epoch 4 Loss 0.5036568589775694 Acc 0.7357369301686811\n",
      "Final accuracy: 0.7053824362606232\n",
      "batch 99 Loss 0.9277128916467802 Acc 0.5096061859527148\n",
      "batch 199 Loss 0.95481907490076 Acc 0.5669114006162072\n",
      "batch 299 Loss 0.9670660877032166 Acc 0.5303266368459493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 399 Loss 0.9105648774827116 Acc 0.5890944461949873\n",
      "batch 499 Loss 0.9242953308702428 Acc 0.5374457755143317\n",
      "batch 599 Loss 0.9446107252212541 Acc 0.5352160922795922\n",
      "batch 699 Loss 1.185294135479346 Acc 0.4492592592278209\n",
      "batch 799 Loss 0.9717739192317059 Acc 0.4521832883485542\n",
      "batch 899 Loss 0.9456283007534574 Acc 0.5351570634587598\n",
      "batch 999 Loss 1.0726272426042402 Acc 0.5283894207011407\n",
      "Epoch 0 Loss 1.0630553625829693 Acc 0.5198020385077372\n",
      "batch 99 Loss 1.0177807434355175 Acc 0.5248114941074811\n",
      "batch 199 Loss 0.9856888085467843 Acc 0.5211989256393522\n",
      "batch 299 Loss 0.9497293796784723 Acc 0.5276814813115606\n",
      "batch 399 Loss 0.8889635934713707 Acc 0.4923713160865309\n",
      "batch 499 Loss 0.8998282124856068 Acc 0.5412592689596639\n",
      "batch 599 Loss 0.8069256593185833 Acc 0.5520810000712962\n",
      "batch 699 Loss 0.8324706128736824 Acc 0.5157741173997502\n",
      "batch 799 Loss 0.9873943505276581 Acc 0.48940703725490764\n",
      "batch 899 Loss 1.002240018997958 Acc 0.5860369276883198\n",
      "batch 999 Loss 1.111102791087572 Acc 0.5240486055627916\n",
      "Epoch 1 Loss 1.0718839357026841 Acc 0.5701047211236606\n",
      "batch 99 Loss 0.8843391735433888 Acc 0.5864882233564728\n",
      "batch 199 Loss 1.0465637882704697 Acc 0.5542635531099376\n",
      "batch 299 Loss 1.0100671622137698 Acc 0.4644546430309034\n",
      "batch 399 Loss 0.9027384624793904 Acc 0.46067398037245966\n",
      "batch 499 Loss 0.8741613671074703 Acc 0.5025417750775626\n",
      "batch 599 Loss 0.8791413647365727 Acc 0.5114830383571065\n",
      "batch 699 Loss 0.8764322394530921 Acc 0.4613743607315717\n",
      "batch 799 Loss 1.0052050402250443 Acc 0.4789770469701968\n",
      "batch 899 Loss 0.9051869176707447 Acc 0.5351797183103417\n",
      "batch 999 Loss 1.0160316058471233 Acc 0.5253421820516383\n",
      "Epoch 2 Loss 1.0044637665983824 Acc 0.4654254703452126\n",
      "batch 99 Loss 0.8993433021006781 Acc 0.4941588268575467\n",
      "batch 199 Loss 0.9802615734765024 Acc 0.5146639059869014\n",
      "batch 299 Loss 0.8959912589116528 Acc 0.5475047813629907\n",
      "batch 399 Loss 0.8640940495300157 Acc 0.47465764340405936\n",
      "batch 499 Loss 0.9795168569746668 Acc 0.47942617109024527\n",
      "batch 599 Loss 0.8742553242757342 Acc 0.5063156141303777\n",
      "batch 699 Loss 1.0048304197424585 Acc 0.5381176936662445\n",
      "batch 799 Loss 0.8664716088463992 Acc 0.5471007354834625\n",
      "batch 899 Loss 0.9142640352591452 Acc 0.5261393083992011\n",
      "batch 999 Loss 0.9573922956986008 Acc 0.49590818659838015\n",
      "Epoch 3 Loss 0.8866334921800227 Acc 0.4993630459785305\n",
      "batch 99 Loss 0.9407427274995158 Acc 0.5322017674170196\n",
      "batch 199 Loss 0.8741948889555166 Acc 0.5223707308968004\n",
      "batch 299 Loss 0.9185703696589659 Acc 0.4978961862919428\n",
      "batch 399 Loss 0.965493053222586 Acc 0.49791127637378013\n",
      "batch 499 Loss 0.9041497118002502 Acc 0.4849017073745902\n",
      "batch 599 Loss 0.8299497674209908 Acc 0.5009528944678698\n",
      "batch 699 Loss 1.0363741975954783 Acc 0.5207835468398817\n",
      "batch 799 Loss 1.045578224587685 Acc 0.5216666847766698\n",
      "batch 899 Loss 1.0528096311315416 Acc 0.48802181910771\n",
      "batch 999 Loss 1.0276622813244285 Acc 0.48765692849937187\n",
      "Epoch 4 Loss 0.9154867131296853 Acc 0.514004162170167\n",
      "Final accuracy: 0.5127478753541076\n",
      "batch 99 Loss 3.1070286384639583 Acc 0.4408373938798713\n",
      "batch 199 Loss 5.8255946745061555 Acc 0.45564759888710155\n",
      "batch 299 Loss 4.880261365865042 Acc 0.5039704847621368\n",
      "batch 399 Loss 4.5612172888087 Acc 0.5445142766831523\n",
      "batch 499 Loss 4.567651597120356 Acc 0.5199805215466503\n",
      "batch 599 Loss 6.342552923596484 Acc 0.49158827592188525\n",
      "batch 699 Loss 6.083596003697407 Acc 0.5012539504116333\n",
      "batch 799 Loss 5.531490419999056 Acc 0.5557579361134044\n",
      "batch 899 Loss 6.904982895208793 Acc 0.47735193105539603\n",
      "batch 999 Loss 5.995996088094903 Acc 0.5248258986560713\n",
      "Epoch 0 Loss 6.331902346582561 Acc 0.47408820419392317\n",
      "batch 99 Loss 5.048200656614284 Acc 0.5212327948732107\n",
      "batch 199 Loss 7.297147652863215 Acc 0.44069325289300343\n",
      "batch 299 Loss 5.691958152554395 Acc 0.5009713529014952\n",
      "batch 399 Loss 5.608290737980047 Acc 0.5061734077633682\n",
      "batch 499 Loss 5.58497555064245 Acc 0.425423094257195\n",
      "batch 599 Loss 4.970529309700084 Acc 0.4867280749078247\n",
      "batch 699 Loss 5.759733232737684 Acc 0.4869786491217045\n",
      "batch 799 Loss 5.853163346423294 Acc 0.4937572075294099\n",
      "batch 899 Loss 5.251197848947343 Acc 0.5110192674062557\n",
      "batch 999 Loss 5.50307708874909 Acc 0.48293917974330025\n",
      "Epoch 1 Loss 5.749619895427516 Acc 0.525170331419023\n",
      "batch 99 Loss 7.06400536726686 Acc 0.47309749221917974\n",
      "batch 199 Loss 7.373597396756143 Acc 0.43456940674558975\n",
      "batch 299 Loss 7.948510041891286 Acc 0.5091447592332806\n",
      "batch 399 Loss 5.94356257484492 Acc 0.49116095627369616\n",
      "batch 499 Loss 6.122243193346143 Acc 0.534549091562625\n",
      "batch 599 Loss 6.231935371768202 Acc 0.5330345693425299\n",
      "batch 699 Loss 4.833275044830068 Acc 0.5545217291194559\n",
      "batch 799 Loss 6.893311875736405 Acc 0.5164278043868104\n",
      "batch 899 Loss 6.867761617068343 Acc 0.4904503024785642\n",
      "batch 999 Loss 5.757775481105883 Acc 0.5703826624190412\n",
      "Epoch 2 Loss 5.5674097111684455 Acc 0.5488176048269533\n",
      "batch 99 Loss 6.2167820412179235 Acc 0.5125360221590489\n",
      "batch 199 Loss 7.5162744089502995 Acc 0.5295826160973146\n",
      "batch 299 Loss 6.478552312163784 Acc 0.46578171691868286\n",
      "batch 399 Loss 6.0841820395052855 Acc 0.5539791560352476\n",
      "batch 499 Loss 5.548354007506616 Acc 0.5573753848071156\n",
      "batch 599 Loss 5.484674104960796 Acc 0.5195505895747239\n",
      "batch 699 Loss 5.552599455679866 Acc 0.5582366739839881\n",
      "batch 799 Loss 6.091305693486303 Acc 0.5069579131036182\n",
      "batch 899 Loss 6.659368608042061 Acc 0.48986220534395436\n",
      "batch 999 Loss 5.814895903260714 Acc 0.5241861549554688\n",
      "Epoch 3 Loss 6.703068108418129 Acc 0.5165693757006777\n",
      "batch 99 Loss 6.876905832103175 Acc 0.5122864841415046\n",
      "batch 199 Loss 5.494561432460875 Acc 0.5573311102641376\n",
      "batch 299 Loss 6.39609752945917 Acc 0.5161797432439577\n",
      "batch 399 Loss 5.317395274053776 Acc 0.5313908399426381\n",
      "batch 499 Loss 5.362694101423852 Acc 0.5212443638168478\n",
      "batch 599 Loss 4.68658177482477 Acc 0.5251673079663177\n",
      "batch 699 Loss 4.647077220488272 Acc 0.6138019668559619\n",
      "batch 799 Loss 4.429276925369556 Acc 0.5813523944886895\n",
      "batch 899 Loss 5.901127886594117 Acc 0.5213460713686598\n",
      "batch 999 Loss 6.936576388626787 Acc 0.5001804592351151\n",
      "Epoch 4 Loss 7.055176035191696 Acc 0.5054893139060579\n",
      "Final accuracy: 0.5410764872521246\n",
      "Max accuracy:  0.9518413597733711\n",
      "max parameters:  [0.001, 25, 50]\n"
     ]
    }
   ],
   "source": [
    "results_acc=[]\n",
    "results_param=[]\n",
    "\n",
    "for n_neurons in [10,20,50,100,200]:\n",
    "    for lr in [0.001, 0.01, 0.1]:\n",
    "        tf.reset_default_graph()\n",
    "        # sizes\n",
    "        n_steps = None\n",
    "        n_inputs = 300\n",
    "        # Build RNN\n",
    "        X= tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "        y= tf.placeholder(tf.float32, [None, 1])\n",
    "        basic_cell = tf.contrib.rnn.BasicRNNCell(n_neurons,activation=tf.nn.tanh)\n",
    "        outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "        last_cell_output=outputs[:,-1,:]\n",
    "        y_=tf.layers.dense(last_cell_output,1)\n",
    "\n",
    "        # Loss and metrics\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_)), y), tf.float32))\n",
    "\n",
    "        # Training\n",
    "        train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "\n",
    "        initialize_all = tf.global_variables_initializer()\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(initialize_all)\n",
    "        l_ma=.74\n",
    "        acc_ma=.5\n",
    "        for epoch in range(5):\n",
    "            for batch in range(train_batches):\n",
    "                data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "                reviews = np.array([sample['w'] for sample in data]).reshape([1,-1,300])\n",
    "                labels  = np.array([sample['y'] for sample in data]).reshape([1,1])\n",
    "                labels = np.array(labels).reshape([-1, 1])\n",
    "                _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "                l_ma=.99*l_ma+(.01)*l\n",
    "                acc_ma=.99*acc_ma+(.01)*acc\n",
    "                if (batch+1) % 100 == 0:\n",
    "                    print(\"batch\", batch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"Epoch\", epoch, \"Loss\", l_ma, \"Acc\", acc_ma)\n",
    "            random.shuffle(train)\n",
    "        # Evaluate on test set\n",
    "        test_acc=0\n",
    "        n=0\n",
    "        for sample in test:\n",
    "            test_reviews = np.array([sample['w'] ]).reshape([1,-1,300])\n",
    "            test_labels  = np.array([sample['y']]).reshape([1,1])\n",
    "            test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "            test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "            n+=1\n",
    "        acc=test_acc/n \n",
    "        print(\"Final accuracy:\", acc)\n",
    "        results_acc.append(acc)\n",
    "        results_param.append([lr, layer_size_1, layer_size_2])\n",
    "argmax=np.argmax(results_acc)\n",
    "print ('Max accuracy: ', results_acc[argmax])\n",
    "print ('max parameters: ', results_param[argmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the hyperparameters the accuracy was 0.95 for 5 epochs.\n",
    "\n",
    "The optimal number of layer is 50:\n",
    "\n",
    "the learning rate is 0.001.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
