{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-0197b70b08e3>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\hp\\anaconda2\\envs\\translator\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load datasets\n",
    "train=mnist.train\n",
    "val=mnist.validation\n",
    "test=mnist.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a 2-layer CNN with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "units=200\n",
    "lr=0.01\n",
    "iterations_n=30000\n",
    "batch_size=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30,000 iterations\n",
    "\n",
    "batch size is 250\n",
    "\n",
    "learning rate is 0.01\n",
    "\n",
    "kernel size is 3\n",
    "\n",
    "neurons in hidden unites are 200.\n",
    "\n",
    "86 filters in each conv layer.\n",
    "\n",
    "Max pooling with kernel size of 2\n",
    "    \n",
    "Keep_prob=0.75 for dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs\n",
    "X= tf.placeholder(tf.float32, [None, 784])\n",
    "x=tf.reshape(X, [-1, 28,28,1])\n",
    "Y=tf.placeholder(tf.int32, [None, 10])\n",
    "\n",
    "#First conv\n",
    "with tf.name_scope('conv1') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3, 1, 86], dtype=tf.float32, stddev=1e-1), name='weights')\n",
    "    bias=tf.Variable(tf.zeros([86]))\n",
    "    conv_1 = tf.nn.conv2d(x, kernel, [1, 1, 1, 1], padding='SAME')+bias\n",
    "    conv_1= tf.nn.relu(conv_1, name=scope)\n",
    "\n",
    "#Pooling\n",
    "pool1 = tf.nn.max_pool(conv_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')    \n",
    "    \n",
    "#second conv\n",
    "with tf.name_scope('conv2') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3, 86, 86], dtype=tf.float32, stddev=1e-1), name='weights')\n",
    "    bias=tf.Variable(tf.zeros([86]))\n",
    "    conv_2 = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    conv_2= tf.nn.relu(conv_2, name=scope)\n",
    "#pooling    \n",
    "pool2 = tf.nn.max_pool(conv_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')     \n",
    "#fully conected \n",
    "with tf.name_scope('ml1') as scope:\n",
    "    #layer1=tf.reshape(conv_2, [-1, 28*28*32])\n",
    "    layer1=tf.contrib.layers.flatten(pool2)\n",
    "    W=tf.Variable(tf.truncated_normal([int(layer1.get_shape()[1]), units]))\n",
    "    b=tf.Variable(tf.zeros([units]))\n",
    "    mul=tf.matmul(layer1, W)+b\n",
    "    ml_1=tf.sigmoid(mul)\n",
    "\n",
    "    \n",
    "dropout=tf.nn.dropout(ml_1, 0.75)\n",
    "#fully conected\n",
    "with tf.name_scope('ml2') as scope:\n",
    "    W=tf.Variable(tf.truncated_normal([units, 10]))\n",
    "    b=tf.Variable(tf.zeros([10]))\n",
    "    y_hat=tf.matmul(dropout, W)+b\n",
    "\n",
    "#Loss function\n",
    "cross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=y_hat))\n",
    "#Optimizer\n",
    "optimizer=tf.train.GradientDescentOptimizer(lr)\n",
    "training_op=optimizer.minimize(cross_entropy)\n",
    "# Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_accs=[0]\n",
    "losses=[]\n",
    "stop=0\n",
    "with tf.Session() as sess:\n",
    "    init=tf.global_variables_initializer()\n",
    "    init.run()\n",
    "    for iters in range(iterations_n):\n",
    "        if stop>3:\n",
    "            print ('EARLY STOP, EPOCH: ', iters)\n",
    "            break\n",
    "        if iters % 250 == 0:\n",
    "            validation_accuracy = 0\n",
    "            for v in range(10):\n",
    "                batch = mnist.validation.next_batch(batch_size)\n",
    "                validation_accuracy += (1/10) * accuracy.eval(feed_dict={X: batch[0], Y: batch[1]})\n",
    "            #print('step %d, validation accuracy %g' % (iters, validation_accuracy))\n",
    "\n",
    "            if validation_accuracy<valid_accs[-1]:\n",
    "                stop+=1\n",
    "            else:\n",
    "                stop=0\n",
    "            valid_accs.append(validation_accuracy)\n",
    "        X_batch, y_batch=train.next_batch(batch_size)\n",
    "        loss,_=sess.run([cross_entropy, training_op], feed_dict={X: X_batch, Y: y_batch})\n",
    "        losses.append(loss)\n",
    "    test_accuracy=accuracy.eval(feed_dict={X: test.images, Y: test.labels})\n",
    "    val_accuracy=accuracy.eval(feed_dict={X: val.images, Y: val.labels})\n",
    "print ('Iterations:', iterations_n)\n",
    "print('Validation accuracy: ', val_accuracy)\n",
    "print ('Test accuracy: ',  test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
